{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "\n",
    "def read_file(fn):\n",
    "    datapoints = list()\n",
    "    lastp = lastm = None\n",
    "    for l in open(fn):\n",
    "        arr = l.split(\" \")\n",
    "        ctime = float(arr[1])\n",
    "        if ctime == -1:\n",
    "            continue\n",
    "        if arr[0] == \"p\":\n",
    "            lastp = arr\n",
    "        if arr[0] == \"m\":\n",
    "            lastm = arr\n",
    "        if lastm and lastp and abs(float(lastm[1])-float(lastp[1]))<0.03:\n",
    "            datapoints.append([float(x) for x in lastp[1:]+lastm[2:]])\n",
    "    inputs=list()\n",
    "    outputs=list()\n",
    "    if len(datapoints)>1:\n",
    "        for i,d in enumerate(datapoints[1:]):\n",
    "            if i==0:\n",
    "                continue\n",
    "            #print(datapoints[i])\n",
    "            #print(d)\n",
    "            #print(\"---\")\n",
    "            if abs(d[0]-datapoints[i][0] - 0.1)<0.05:\n",
    "                inputs.append(datapoints[i-1][1:-4] + datapoints[i][1:])\n",
    "                # transform outputs into deltas\n",
    "                outputs.append([result-start for (result, start) in zip(d[1:8],datapoints[i][1:8])])\n",
    "                if outputs[-1][-1]>15:\n",
    "                    print(d[1:8])\n",
    "                    print(datapoints[i][1:8])\n",
    "\n",
    "    return inputs, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw/good/coords-2019-5-29-16-48-40.txt\n",
      "[0.6166287375722932, 0.20703949106010078, -0.4595463732612331, 0.604748412258812, -96.063914, 182.957699, -995.656336]\n",
      "[0.6182501516376684, 0.20117099796991333, -0.4563298516833082, 0.6075028837520084, -97.041789, 184.829208, -1013.008333]\n",
      "raw/good/coords-2019-5-29-16-17-5.txt\n",
      "raw/good/coords-2019-5-29-16-9-46.txt\n",
      "raw/good/coords-2019-5-29-16-48-2.txt\n",
      "[0.6207429016911913, 0.20762863280250207, -0.4615252131268397, 0.5988010478852241, -86.622843, 174.248154, -1005.282454]\n",
      "[0.6157473101849492, 0.20834276963624446, -0.4551254148651092, 0.6085308759001363, -88.091356, 176.072826, -1020.454659]\n",
      "[0.6138063619741978, 0.2143505804938703, -0.4557756604219752, 0.6079176970402362, -93.13703, 179.285656, -994.248057]\n",
      "[0.6179010843168994, 0.2089488969625828, -0.45580313281268864, 0.6056252521610364, -94.277137, 181.266333, -1012.997424]\n",
      "raw/good/coords-2019-5-29-16-52-3.txt\n",
      "[0.6683520778751272, 0.1394134215849763, -0.48744181515190604, 0.5443066791332234, 48.675976, 95.446029, -1024.306946]\n",
      "[0.676046780925699, 0.13629345276052246, -0.48013393326944115, 0.5420848975838516, 50.110647, 97.449976, -1043.712146]\n",
      "raw/good/coords-2019-5-29-16-10-36.txt\n",
      "[0.5435329703319369, 0.2659361387893294, -0.4542783511482183, 0.6538205028905104, -59.420826, -33.2085, -947.219688]\n",
      "[0.5517299184432698, 0.26413039514708825, -0.4448208609904566, 0.6541891928181021, -60.472656, -33.462651, -962.340819]\n",
      "raw/good/coords-2019-5-29-16-49-59.txt\n",
      "[0.6464406391928031, 0.22021240833149777, -0.4437371857657081, 0.5802744710920336, 26.544338, 252.654544, -1008.483875]\n",
      "[0.6485761327708567, 0.21151217731976973, -0.44042208087376494, 0.5836435090246189, 26.494187, 256.934786, -1024.637659]\n",
      "[0.6457094935030767, 0.22298611287076253, -0.4405053710096096, 0.5824875641203622, 29.725467, 250.052336, -1005.349963]\n",
      "[0.6474395724698947, 0.21780449635175347, -0.439785444244281, 0.5830710170524116, 29.765996, 253.059976, -1022.003295]\n",
      "raw/good/coords-2019-5-29-16-12-55.txt\n",
      "raw/good/coords-2019-5-21-16-8-35.txt\n",
      "raw/good/coords-2019-5-21-16-9-48.txt\n",
      "[0.6410237905101495, 0.17709171122908984, -0.4666301070697375, 0.5830809332404676, -87.791602, 261.882928, -880.384953]\n",
      "[0.6433160187652722, 0.17129217489640508, -0.4646623763134822, 0.5838600144310229, -89.665835, 262.568685, -896.483644]\n",
      "raw/good/coords-2019-5-29-16-11-47.txt\n",
      "raw/good/coords-2019-5-29-16-8-51.txt\n",
      "raw/manual/coords-2019-5-29-16-51-24.txt\n",
      "[0.6250513978866058, 0.24791481232414125, -0.4429935697067794, 0.5929640366426933, 38.808409, 163.89066, -1031.331994]\n",
      "[0.6236182726636543, 0.24552479731872956, -0.44016750636177787, 0.5975538503840226, 38.819298, 167.120993, -1047.671396]\n",
      "[0.5900615712205871, 0.25226067008027997, -0.4521296334841683, 0.6194921307006247, 22.406547, 91.166103, -1009.795796]\n",
      "[0.5916966026397166, 0.2503631552154289, -0.4476555165813852, 0.6219475460197588, 22.226686, 93.150414, -1025.673478]\n",
      "raw/manual/coords-2019-5-29-16-22-33.txt\n",
      "[0.6288572970078348, 0.18183616624007365, -0.4730484983086613, 0.5896604074793462, -135.273177, 250.745628, -955.605426]\n",
      "[0.6326122430051444, 0.16844362589918047, -0.46861051659664005, 0.593154818214526, -137.792717, 254.309383, -973.018069]\n",
      "raw/manual/coords-2019-5-29-16-49-19.txt\n",
      "[0.6207918330648367, 0.22791738303236123, -0.45343339748897915, 0.5975533669130223, -28.15245, 188.54244, -1013.457108]\n",
      "[0.626223801847231, 0.22252308134719329, -0.44542901304164706, 0.5999336800865506, -29.425747, 192.082573, -1029.203226]\n",
      "raw/manual/coords-2019-5-29-16-19-40.txt\n",
      "[0.6732261878447687, 0.1719466682818516, -0.45920599284721086, 0.5534714880787079, 107.249172, 6.107221, -901.834864]\n",
      "[0.6694716200108859, 0.16407865056059262, -0.4719539119445606, 0.5496775203017811, 109.153104, 5.615618, -923.994273]\n",
      "[0.6855191463409318, 0.15043781132950435, -0.46811311064447686, 0.5369375924285867, 135.378801, -26.566823, -970.990165]\n",
      "[0.6770269566272823, 0.13746239952338363, -0.4843695613445612, 0.5367722015241182, 139.032794, -27.05607, -1016.040502]\n",
      "[0.6792129636572023, 0.1660683114654564, -0.45413849043623034, 0.5521316878004547, 132.42046, -26.197611, -941.647595]\n",
      "[0.6855191463409318, 0.15043781132950435, -0.46811311064447686, 0.5369375924285867, 135.378801, -26.566823, -970.990165]\n",
      "[0.6770485580813239, 0.1549766981224361, -0.4698924415429985, 0.5447829341004166, 140.50851, -32.450238, -983.949232]\n",
      "[0.6857752911850937, 0.14255544236809475, -0.46928856162759836, 0.5377348159668074, 142.309252, -31.903337, -1000.923113]\n",
      "[0.677988569225175, 0.16775540940163797, -0.4560087942977077, 0.551584520705683, 138.850642, -32.221616, -961.179641]\n",
      "[0.6770485580813239, 0.1549766981224361, -0.4698924415429985, 0.5447829341004166, 140.50851, -32.450238, -983.949232]\n",
      "raw/manual/coords-2019-5-29-16-19-29.txt\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "ins = list()\n",
    "outs = list()\n",
    "\n",
    "for fn in listdir(\"raw/good\"):\n",
    "    ffn=\"raw/good/\"+fn\n",
    "    print(ffn)\n",
    "    i,o = read_file(ffn)\n",
    "    ins+=i\n",
    "    outs+=o\n",
    "for fn in listdir(\"raw/manual/\"):\n",
    "    ffn=\"raw/manual/\"+fn\n",
    "    print(ffn)\n",
    "    i,o = read_file(ffn)\n",
    "    ins+=i\n",
    "    outs+=o\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ins[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input \"normalization\"\n",
    "for l in ins:\n",
    "    l[4]/=1000.0\n",
    "    l[5]/=1000.0\n",
    "    l[6]/=1000.0\n",
    "    l[7+4]/=1000.0\n",
    "    l[7+5]/=1000.0\n",
    "    l[7+6]/=1000.0\n",
    "    l[7+7]/=1000.0\n",
    "    l[7+8]/=1000.0\n",
    "    l[7+9]/=1000.0\n",
    "    l[7+10]/=1000.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output \"normalization\"\n",
    "for l in outs:\n",
    "    l[0]*=100.0\n",
    "    l[1]*=100.0\n",
    "    l[2]*=100.0\n",
    "    l[3]*=100.0\n",
    "    l[4]/=10.0\n",
    "    l[5]/=10.0\n",
    "    l[6]/=10.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6400 6400\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "print(len(ins), len(outs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = np.array([[o[4],o[5],o[6]] for o in outs])\n",
    "inputs = np.array(ins)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.5748909, -0.5498622, -4.8350689])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.amin(outputs,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/yves/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/yves/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/yves/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/yves/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/yves/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/yves/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/yves/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/yves/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/yves/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/yves/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/yves/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/yves/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import Callback\n",
    "\n",
    "class NBatchLogger(Callback):\n",
    "    \"\"\"\n",
    "    A Logger that log average performance per `display` steps.\n",
    "    \"\"\"\n",
    "    def __init__(self, display):\n",
    "        self.step = 0\n",
    "        self.display = display\n",
    "        self.metric_cache = {}\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.step += 1\n",
    "        for k in self.params['metrics']:\n",
    "            if k in logs:\n",
    "                self.metric_cache[k] = self.metric_cache.get(k, 0) + logs[k]\n",
    "        if self.step % self.display == 0:\n",
    "            metrics_log = ''\n",
    "            for (k, v) in self.metric_cache.items():\n",
    "                val = v / self.display\n",
    "                if abs(val) > 1e-3:\n",
    "                    metrics_log += ' - %s: %.4f' % (k, val)\n",
    "                else:\n",
    "                    metrics_log += ' - %s: %.4e' % (k, val)\n",
    "            print('step: {}/{} ... {}'.format(self.step,\n",
    "                                          self.params['epochs'],\n",
    "                                          metrics_log))\n",
    "            self.metric_cache.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 160)               3040      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 160)               25760     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 3)                 483       \n",
      "=================================================================\n",
      "Total params: 29,283\n",
      "Trainable params: 29,283\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "step: 100/2000 ...  - loss: 0.0975 - acc: 0.3755\n",
      "step: 200/2000 ...  - loss: 0.0739 - acc: 0.4263\n",
      "step: 300/2000 ...  - loss: 0.0739 - acc: 0.4289\n",
      "step: 400/2000 ...  - loss: 0.0737 - acc: 0.4322\n",
      "step: 500/2000 ...  - loss: 0.0735 - acc: 0.4423\n",
      "step: 600/2000 ...  - loss: 0.0732 - acc: 0.4494\n",
      "step: 700/2000 ...  - loss: 0.0729 - acc: 0.4508\n",
      "step: 800/2000 ...  - loss: 0.0726 - acc: 0.4510\n",
      "step: 900/2000 ...  - loss: 0.0722 - acc: 0.4553\n",
      "step: 1000/2000 ...  - loss: 0.0721 - acc: 0.4569\n",
      "step: 1100/2000 ...  - loss: 0.0717 - acc: 0.4608\n",
      "step: 1200/2000 ...  - loss: 0.0712 - acc: 0.4676\n",
      "step: 1300/2000 ...  - loss: 0.0704 - acc: 0.4735\n",
      "step: 1400/2000 ...  - loss: 0.0701 - acc: 0.4774\n",
      "step: 1500/2000 ...  - loss: 0.0698 - acc: 0.4819\n",
      "step: 1600/2000 ...  - loss: 0.0693 - acc: 0.4847\n",
      "step: 1700/2000 ...  - loss: 0.0714 - acc: 0.4766\n",
      "step: 1800/2000 ...  - loss: 0.0711 - acc: 0.4894\n",
      "step: 1900/2000 ...  - loss: 0.0696 - acc: 0.4816\n",
      "step: 2000/2000 ...  - loss: 0.0673 - acc: 0.5029\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fba50403b70>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras import backend as K\n",
    "from keras.layers import LeakyReLU\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(160, activation='tanh', input_shape=(18,), use_bias=True))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(160, activation='tanh', use_bias=True))\n",
    "#model.add(Dense(6000, activation='sigmoid', input_shape=(32,)))\n",
    "#model.add(Dense(160, activation='relu', input_shape=(32,)))\n",
    "#model.add(Dense(80, activation='sigmoid'))\n",
    "#model.add(Dense(40, activation='relu'))\n",
    "#model.add(Dense(20, activation='linear'))\n",
    "model.add(Dense(3, activation='linear'))\n",
    "\n",
    "model.summary()\n",
    "#opt = SGD(lr=0.08, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "opt = Adam(lr=0.004, amsgrad=True)\n",
    "model.compile(loss='mean_squared_error',\n",
    "  optimizer=opt,\n",
    "  metrics=['accuracy'])\n",
    "\n",
    "model.fit(inputs, outputs,\n",
    "    batch_size=6400,\n",
    "    epochs=2000,\n",
    "    verbose=0,\n",
    "    shuffle=True,\n",
    "    validation_split=0.0,\n",
    "    callbacks=[NBatchLogger(display=100)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 100/50000 ...  - loss: 0.0489 - acc: 0.5887\n",
      "step: 200/50000 ...  - loss: 0.0491 - acc: 0.5874\n",
      "step: 300/50000 ...  - loss: 0.0493 - acc: 0.5867\n",
      "step: 400/50000 ...  - loss: 0.0491 - acc: 0.5870\n",
      "step: 500/50000 ...  - loss: 0.0493 - acc: 0.5861\n",
      "step: 600/50000 ...  - loss: 0.0492 - acc: 0.5868\n",
      "step: 700/50000 ...  - loss: 0.0490 - acc: 0.5873\n",
      "step: 800/50000 ...  - loss: 0.0493 - acc: 0.5861\n",
      "step: 900/50000 ...  - loss: 0.0490 - acc: 0.5871\n",
      "step: 1000/50000 ...  - loss: 0.0490 - acc: 0.5879\n",
      "step: 1100/50000 ...  - loss: 0.0492 - acc: 0.5871\n",
      "step: 1200/50000 ...  - loss: 0.0492 - acc: 0.5871\n",
      "step: 1300/50000 ...  - loss: 0.0490 - acc: 0.5882\n",
      "step: 1400/50000 ...  - loss: 0.0492 - acc: 0.5864\n",
      "step: 1500/50000 ...  - loss: 0.0490 - acc: 0.5875\n",
      "step: 1600/50000 ...  - loss: 0.0491 - acc: 0.5877\n",
      "step: 1700/50000 ...  - loss: 0.0491 - acc: 0.5862\n",
      "step: 1800/50000 ...  - loss: 0.0490 - acc: 0.5869\n",
      "step: 1900/50000 ...  - loss: 0.0491 - acc: 0.5864\n",
      "step: 2000/50000 ...  - loss: 0.0490 - acc: 0.5873\n",
      "step: 2100/50000 ...  - loss: 0.0488 - acc: 0.5894\n",
      "step: 2200/50000 ...  - loss: 0.0492 - acc: 0.5859\n",
      "step: 2300/50000 ...  - loss: 0.0488 - acc: 0.5885\n",
      "step: 2400/50000 ...  - loss: 0.0489 - acc: 0.5880\n",
      "step: 2500/50000 ...  - loss: 0.0494 - acc: 0.5853\n",
      "step: 2600/50000 ...  - loss: 0.0488 - acc: 0.5876\n",
      "step: 2700/50000 ...  - loss: 0.0490 - acc: 0.5877\n",
      "step: 2800/50000 ...  - loss: 0.0490 - acc: 0.5870\n",
      "step: 2900/50000 ...  - loss: 0.0489 - acc: 0.5883\n",
      "step: 3000/50000 ...  - loss: 0.0491 - acc: 0.5866\n",
      "step: 3100/50000 ...  - loss: 0.0491 - acc: 0.5868\n",
      "step: 3200/50000 ...  - loss: 0.0490 - acc: 0.5873\n",
      "step: 3300/50000 ...  - loss: 0.0490 - acc: 0.5873\n",
      "step: 3400/50000 ...  - loss: 0.0488 - acc: 0.5891\n",
      "step: 3500/50000 ...  - loss: 0.0490 - acc: 0.5866\n",
      "step: 3600/50000 ...  - loss: 0.0487 - acc: 0.5890\n",
      "step: 3700/50000 ...  - loss: 0.0489 - acc: 0.5872\n",
      "step: 3800/50000 ...  - loss: 0.0488 - acc: 0.5890\n",
      "step: 3900/50000 ...  - loss: 0.0490 - acc: 0.5872\n",
      "step: 4000/50000 ...  - loss: 0.0488 - acc: 0.5886\n",
      "step: 4100/50000 ...  - loss: 0.0488 - acc: 0.5884\n",
      "step: 4200/50000 ...  - loss: 0.0489 - acc: 0.5883\n",
      "step: 4300/50000 ...  - loss: 0.0487 - acc: 0.5895\n",
      "step: 4400/50000 ...  - loss: 0.0488 - acc: 0.5877\n",
      "step: 4500/50000 ...  - loss: 0.0489 - acc: 0.5877\n",
      "step: 4600/50000 ...  - loss: 0.0488 - acc: 0.5886\n",
      "step: 4700/50000 ...  - loss: 0.0489 - acc: 0.5887\n",
      "step: 4800/50000 ...  - loss: 0.0487 - acc: 0.5887\n",
      "step: 4900/50000 ...  - loss: 0.0487 - acc: 0.5897\n",
      "step: 5000/50000 ...  - loss: 0.0489 - acc: 0.5878\n",
      "step: 5100/50000 ...  - loss: 0.0489 - acc: 0.5873\n",
      "step: 5200/50000 ...  - loss: 0.0486 - acc: 0.5890\n",
      "step: 5300/50000 ...  - loss: 0.0488 - acc: 0.5882\n",
      "step: 5400/50000 ...  - loss: 0.0486 - acc: 0.5889\n",
      "step: 5500/50000 ...  - loss: 0.0488 - acc: 0.5887\n",
      "step: 5600/50000 ...  - loss: 0.0487 - acc: 0.5885\n",
      "step: 5700/50000 ...  - loss: 0.0486 - acc: 0.5888\n",
      "step: 5800/50000 ...  - loss: 0.0488 - acc: 0.5876\n",
      "step: 5900/50000 ...  - loss: 0.0486 - acc: 0.5890\n",
      "step: 6000/50000 ...  - loss: 0.0488 - acc: 0.5875\n",
      "step: 6100/50000 ...  - loss: 0.0487 - acc: 0.5879\n",
      "step: 6200/50000 ...  - loss: 0.0488 - acc: 0.5877\n",
      "step: 6300/50000 ...  - loss: 0.0485 - acc: 0.5900\n",
      "step: 6400/50000 ...  - loss: 0.0487 - acc: 0.5885\n",
      "step: 6500/50000 ...  - loss: 0.0486 - acc: 0.5889\n",
      "step: 6600/50000 ...  - loss: 0.0486 - acc: 0.5889\n",
      "step: 6700/50000 ...  - loss: 0.0487 - acc: 0.5890\n",
      "step: 6800/50000 ...  - loss: 0.0486 - acc: 0.5890\n",
      "step: 6900/50000 ...  - loss: 0.0484 - acc: 0.5899\n",
      "step: 7000/50000 ...  - loss: 0.0488 - acc: 0.5864\n",
      "step: 7100/50000 ...  - loss: 0.0485 - acc: 0.5894\n",
      "step: 7200/50000 ...  - loss: 0.0485 - acc: 0.5896\n",
      "step: 7300/50000 ...  - loss: 0.0485 - acc: 0.5886\n",
      "step: 7400/50000 ...  - loss: 0.0485 - acc: 0.5892\n",
      "step: 7500/50000 ...  - loss: 0.0487 - acc: 0.5880\n",
      "step: 7600/50000 ...  - loss: 0.0486 - acc: 0.5883\n",
      "step: 7700/50000 ...  - loss: 0.0485 - acc: 0.5894\n",
      "step: 7800/50000 ...  - loss: 0.0484 - acc: 0.5897\n",
      "step: 7900/50000 ...  - loss: 0.0485 - acc: 0.5895\n",
      "step: 8000/50000 ...  - loss: 0.0485 - acc: 0.5896\n",
      "step: 8100/50000 ...  - loss: 0.0486 - acc: 0.5881\n",
      "step: 8200/50000 ...  - loss: 0.0487 - acc: 0.5881\n",
      "step: 8300/50000 ...  - loss: 0.0483 - acc: 0.5910\n",
      "step: 8400/50000 ...  - loss: 0.0486 - acc: 0.5882\n",
      "step: 8500/50000 ...  - loss: 0.0484 - acc: 0.5890\n",
      "step: 8600/50000 ...  - loss: 0.0483 - acc: 0.5903\n",
      "step: 8700/50000 ...  - loss: 0.0486 - acc: 0.5894\n",
      "step: 8800/50000 ...  - loss: 0.0485 - acc: 0.5890\n",
      "step: 8900/50000 ...  - loss: 0.0486 - acc: 0.5884\n",
      "step: 9000/50000 ...  - loss: 0.0487 - acc: 0.5875\n",
      "step: 9100/50000 ...  - loss: 0.0482 - acc: 0.5902\n",
      "step: 9200/50000 ...  - loss: 0.0483 - acc: 0.5906\n",
      "step: 9300/50000 ...  - loss: 0.0482 - acc: 0.5905\n",
      "step: 9400/50000 ...  - loss: 0.0484 - acc: 0.5888\n",
      "step: 9500/50000 ...  - loss: 0.0485 - acc: 0.5881\n",
      "step: 9600/50000 ...  - loss: 0.0484 - acc: 0.5902\n",
      "step: 9700/50000 ...  - loss: 0.0484 - acc: 0.5892\n",
      "step: 9800/50000 ...  - loss: 0.0481 - acc: 0.5911\n",
      "step: 9900/50000 ...  - loss: 0.0483 - acc: 0.5901\n",
      "step: 10000/50000 ...  - loss: 0.0482 - acc: 0.5899\n",
      "step: 10100/50000 ...  - loss: 0.0483 - acc: 0.5887\n",
      "step: 10200/50000 ...  - loss: 0.0482 - acc: 0.5902\n",
      "step: 10300/50000 ...  - loss: 0.0481 - acc: 0.5901\n",
      "step: 10400/50000 ...  - loss: 0.0482 - acc: 0.5901\n",
      "step: 10500/50000 ...  - loss: 0.0484 - acc: 0.5896\n",
      "step: 10600/50000 ...  - loss: 0.0482 - acc: 0.5907\n",
      "step: 10700/50000 ...  - loss: 0.0484 - acc: 0.5884\n",
      "step: 10800/50000 ...  - loss: 0.0483 - acc: 0.5887\n",
      "step: 10900/50000 ...  - loss: 0.0481 - acc: 0.5908\n",
      "step: 11000/50000 ...  - loss: 0.0482 - acc: 0.5900\n",
      "step: 11100/50000 ...  - loss: 0.0483 - acc: 0.5894\n",
      "step: 11200/50000 ...  - loss: 0.0482 - acc: 0.5897\n",
      "step: 11300/50000 ...  - loss: 0.0483 - acc: 0.5890\n",
      "step: 11400/50000 ...  - loss: 0.0481 - acc: 0.5911\n",
      "step: 11500/50000 ...  - loss: 0.0480 - acc: 0.5911\n",
      "step: 11600/50000 ...  - loss: 0.0481 - acc: 0.5900\n",
      "step: 11700/50000 ...  - loss: 0.0480 - acc: 0.5900\n",
      "step: 11800/50000 ...  - loss: 0.0482 - acc: 0.5880\n",
      "step: 11900/50000 ...  - loss: 0.0481 - acc: 0.5907\n",
      "step: 12000/50000 ...  - loss: 0.0482 - acc: 0.5898\n",
      "step: 12100/50000 ...  - loss: 0.0483 - acc: 0.5888\n",
      "step: 12200/50000 ...  - loss: 0.0481 - acc: 0.5911\n",
      "step: 12300/50000 ...  - loss: 0.0482 - acc: 0.5895\n",
      "step: 12400/50000 ...  - loss: 0.0482 - acc: 0.5897\n",
      "step: 12500/50000 ...  - loss: 0.0480 - acc: 0.5907\n",
      "step: 12600/50000 ...  - loss: 0.0481 - acc: 0.5893\n",
      "step: 12700/50000 ...  - loss: 0.0482 - acc: 0.5897\n",
      "step: 12800/50000 ...  - loss: 0.0482 - acc: 0.5887\n",
      "step: 12900/50000 ...  - loss: 0.0481 - acc: 0.5899\n",
      "step: 13000/50000 ...  - loss: 0.0479 - acc: 0.5905\n",
      "step: 13100/50000 ...  - loss: 0.0479 - acc: 0.5904\n",
      "step: 13200/50000 ...  - loss: 0.0481 - acc: 0.5903\n",
      "step: 13300/50000 ...  - loss: 0.0479 - acc: 0.5911\n",
      "step: 13400/50000 ...  - loss: 0.0479 - acc: 0.5904\n",
      "step: 13500/50000 ...  - loss: 0.0480 - acc: 0.5902\n",
      "step: 13600/50000 ...  - loss: 0.0480 - acc: 0.5899\n",
      "step: 13700/50000 ...  - loss: 0.0482 - acc: 0.5884\n",
      "step: 13800/50000 ...  - loss: 0.0481 - acc: 0.5894\n",
      "step: 13900/50000 ...  - loss: 0.0478 - acc: 0.5913\n",
      "step: 14000/50000 ...  - loss: 0.0478 - acc: 0.5901\n",
      "step: 14100/50000 ...  - loss: 0.0479 - acc: 0.5897\n",
      "step: 14200/50000 ...  - loss: 0.0479 - acc: 0.5899\n",
      "step: 14300/50000 ...  - loss: 0.0479 - acc: 0.5898\n",
      "step: 14400/50000 ...  - loss: 0.0477 - acc: 0.5906\n",
      "step: 14500/50000 ...  - loss: 0.0479 - acc: 0.5910\n",
      "step: 14600/50000 ...  - loss: 0.0478 - acc: 0.5908\n",
      "step: 14700/50000 ...  - loss: 0.0479 - acc: 0.5891\n",
      "step: 14800/50000 ...  - loss: 0.0477 - acc: 0.5916\n",
      "step: 14900/50000 ...  - loss: 0.0480 - acc: 0.5889\n",
      "step: 15000/50000 ...  - loss: 0.0478 - acc: 0.5901\n",
      "step: 15100/50000 ...  - loss: 0.0477 - acc: 0.5915\n",
      "step: 15200/50000 ...  - loss: 0.0480 - acc: 0.5894\n",
      "step: 15300/50000 ...  - loss: 0.0479 - acc: 0.5900\n",
      "step: 15400/50000 ...  - loss: 0.0478 - acc: 0.5908\n",
      "step: 15500/50000 ...  - loss: 0.0479 - acc: 0.5890\n",
      "step: 15600/50000 ...  - loss: 0.0477 - acc: 0.5906\n",
      "step: 15700/50000 ...  - loss: 0.0477 - acc: 0.5904\n",
      "step: 15800/50000 ...  - loss: 0.0476 - acc: 0.5909\n",
      "step: 15900/50000 ...  - loss: 0.0478 - acc: 0.5895\n",
      "step: 16000/50000 ...  - loss: 0.0476 - acc: 0.5912\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 16100/50000 ...  - loss: 0.0478 - acc: 0.5898\n",
      "step: 16200/50000 ...  - loss: 0.0480 - acc: 0.5882\n",
      "step: 16300/50000 ...  - loss: 0.0477 - acc: 0.5916\n",
      "step: 16400/50000 ...  - loss: 0.0478 - acc: 0.5898\n",
      "step: 16500/50000 ...  - loss: 0.0477 - acc: 0.5900\n",
      "step: 16600/50000 ...  - loss: 0.0478 - acc: 0.5902\n",
      "step: 16700/50000 ...  - loss: 0.0477 - acc: 0.5900\n",
      "step: 16800/50000 ...  - loss: 0.0475 - acc: 0.5917\n",
      "step: 16900/50000 ...  - loss: 0.0477 - acc: 0.5906\n",
      "step: 17000/50000 ...  - loss: 0.0478 - acc: 0.5897\n",
      "step: 17100/50000 ...  - loss: 0.0477 - acc: 0.5898\n",
      "step: 17200/50000 ...  - loss: 0.0476 - acc: 0.5894\n",
      "step: 17300/50000 ...  - loss: 0.0474 - acc: 0.5917\n",
      "step: 17400/50000 ...  - loss: 0.0476 - acc: 0.5907\n",
      "step: 17500/50000 ...  - loss: 0.0477 - acc: 0.5897\n",
      "step: 17600/50000 ...  - loss: 0.0475 - acc: 0.5916\n",
      "step: 17700/50000 ...  - loss: 0.0477 - acc: 0.5903\n",
      "step: 17800/50000 ...  - loss: 0.0475 - acc: 0.5915\n",
      "step: 17900/50000 ...  - loss: 0.0474 - acc: 0.5907\n",
      "step: 18000/50000 ...  - loss: 0.0477 - acc: 0.5896\n",
      "step: 18100/50000 ...  - loss: 0.0474 - acc: 0.5918\n",
      "step: 18200/50000 ...  - loss: 0.0475 - acc: 0.5903\n",
      "step: 18300/50000 ...  - loss: 0.0475 - acc: 0.5903\n",
      "step: 18400/50000 ...  - loss: 0.0476 - acc: 0.5903\n",
      "step: 18500/50000 ...  - loss: 0.0475 - acc: 0.5901\n",
      "step: 18600/50000 ...  - loss: 0.0473 - acc: 0.5918\n",
      "step: 18700/50000 ...  - loss: 0.0477 - acc: 0.5898\n",
      "step: 18800/50000 ...  - loss: 0.0475 - acc: 0.5906\n",
      "step: 18900/50000 ...  - loss: 0.0475 - acc: 0.5909\n",
      "step: 19000/50000 ...  - loss: 0.0474 - acc: 0.5914\n",
      "step: 19100/50000 ...  - loss: 0.0478 - acc: 0.5888\n",
      "step: 19200/50000 ...  - loss: 0.0474 - acc: 0.5915\n",
      "step: 19300/50000 ...  - loss: 0.0474 - acc: 0.5910\n",
      "step: 19400/50000 ...  - loss: 0.0471 - acc: 0.5929\n",
      "step: 19500/50000 ...  - loss: 0.0475 - acc: 0.5908\n",
      "step: 19600/50000 ...  - loss: 0.0476 - acc: 0.5897\n",
      "step: 19700/50000 ...  - loss: 0.0474 - acc: 0.5912\n",
      "step: 19800/50000 ...  - loss: 0.0473 - acc: 0.5917\n",
      "step: 19900/50000 ...  - loss: 0.0475 - acc: 0.5901\n",
      "step: 20000/50000 ...  - loss: 0.0474 - acc: 0.5906\n",
      "step: 20100/50000 ...  - loss: 0.0475 - acc: 0.5898\n",
      "step: 20200/50000 ...  - loss: 0.0473 - acc: 0.5925\n",
      "step: 20300/50000 ...  - loss: 0.0472 - acc: 0.5921\n",
      "step: 20400/50000 ...  - loss: 0.0472 - acc: 0.5919\n",
      "step: 20500/50000 ...  - loss: 0.0474 - acc: 0.5906\n",
      "step: 20600/50000 ...  - loss: 0.0473 - acc: 0.5915\n",
      "step: 20700/50000 ...  - loss: 0.0472 - acc: 0.5922\n",
      "step: 20800/50000 ...  - loss: 0.0471 - acc: 0.5927\n",
      "step: 20900/50000 ...  - loss: 0.0473 - acc: 0.5920\n",
      "step: 21000/50000 ...  - loss: 0.0472 - acc: 0.5913\n",
      "step: 21100/50000 ...  - loss: 0.0471 - acc: 0.5919\n",
      "step: 21200/50000 ...  - loss: 0.0474 - acc: 0.5904\n",
      "step: 21300/50000 ...  - loss: 0.0473 - acc: 0.5913\n",
      "step: 21400/50000 ...  - loss: 0.0473 - acc: 0.5909\n",
      "step: 21500/50000 ...  - loss: 0.0473 - acc: 0.5912\n",
      "step: 21600/50000 ...  - loss: 0.0471 - acc: 0.5921\n",
      "step: 21700/50000 ...  - loss: 0.0474 - acc: 0.5905\n",
      "step: 21800/50000 ...  - loss: 0.0470 - acc: 0.5931\n",
      "step: 21900/50000 ...  - loss: 0.0472 - acc: 0.5919\n",
      "step: 22000/50000 ...  - loss: 0.0472 - acc: 0.5915\n",
      "step: 22100/50000 ...  - loss: 0.0471 - acc: 0.5921\n",
      "step: 22200/50000 ...  - loss: 0.0471 - acc: 0.5918\n",
      "step: 22300/50000 ...  - loss: 0.0471 - acc: 0.5913\n",
      "step: 22400/50000 ...  - loss: 0.0471 - acc: 0.5917\n",
      "step: 22500/50000 ...  - loss: 0.0471 - acc: 0.5913\n",
      "step: 22600/50000 ...  - loss: 0.0470 - acc: 0.5926\n",
      "step: 22700/50000 ...  - loss: 0.0471 - acc: 0.5918\n",
      "step: 22800/50000 ...  - loss: 0.0473 - acc: 0.5904\n",
      "step: 22900/50000 ...  - loss: 0.0471 - acc: 0.5912\n",
      "step: 23000/50000 ...  - loss: 0.0473 - acc: 0.5909\n",
      "step: 23100/50000 ...  - loss: 0.0471 - acc: 0.5919\n",
      "step: 23200/50000 ...  - loss: 0.0469 - acc: 0.5934\n",
      "step: 23300/50000 ...  - loss: 0.0472 - acc: 0.5906\n",
      "step: 23400/50000 ...  - loss: 0.0472 - acc: 0.5914\n",
      "step: 23500/50000 ...  - loss: 0.0473 - acc: 0.5902\n",
      "step: 23600/50000 ...  - loss: 0.0471 - acc: 0.5922\n",
      "step: 23700/50000 ...  - loss: 0.0469 - acc: 0.5932\n",
      "step: 23800/50000 ...  - loss: 0.0470 - acc: 0.5925\n",
      "step: 23900/50000 ...  - loss: 0.0470 - acc: 0.5927\n",
      "step: 24000/50000 ...  - loss: 0.0469 - acc: 0.5926\n",
      "step: 24100/50000 ...  - loss: 0.0470 - acc: 0.5917\n",
      "step: 24200/50000 ...  - loss: 0.0470 - acc: 0.5922\n",
      "step: 24300/50000 ...  - loss: 0.0467 - acc: 0.5931\n",
      "step: 24400/50000 ...  - loss: 0.0472 - acc: 0.5915\n",
      "step: 24500/50000 ...  - loss: 0.0470 - acc: 0.5930\n",
      "step: 24600/50000 ...  - loss: 0.0471 - acc: 0.5914\n",
      "step: 24700/50000 ...  - loss: 0.0471 - acc: 0.5915\n",
      "step: 24800/50000 ...  - loss: 0.0469 - acc: 0.5926\n",
      "step: 24900/50000 ...  - loss: 0.0468 - acc: 0.5930\n",
      "step: 25000/50000 ...  - loss: 0.0469 - acc: 0.5932\n",
      "step: 25100/50000 ...  - loss: 0.0470 - acc: 0.5914\n",
      "step: 25200/50000 ...  - loss: 0.0469 - acc: 0.5928\n",
      "step: 25300/50000 ...  - loss: 0.0469 - acc: 0.5932\n",
      "step: 25400/50000 ...  - loss: 0.0469 - acc: 0.5930\n",
      "step: 25500/50000 ...  - loss: 0.0470 - acc: 0.5916\n",
      "step: 25600/50000 ...  - loss: 0.0467 - acc: 0.5940\n",
      "step: 25700/50000 ...  - loss: 0.0469 - acc: 0.5925\n",
      "step: 25800/50000 ...  - loss: 0.0467 - acc: 0.5941\n",
      "step: 25900/50000 ...  - loss: 0.0468 - acc: 0.5936\n",
      "step: 26000/50000 ...  - loss: 0.0468 - acc: 0.5923\n",
      "step: 26100/50000 ...  - loss: 0.0466 - acc: 0.5939\n",
      "step: 26200/50000 ...  - loss: 0.0468 - acc: 0.5928\n",
      "step: 26300/50000 ...  - loss: 0.0470 - acc: 0.5923\n",
      "step: 26400/50000 ...  - loss: 0.0468 - acc: 0.5923\n",
      "step: 26500/50000 ...  - loss: 0.0469 - acc: 0.5937\n",
      "step: 26600/50000 ...  - loss: 0.0468 - acc: 0.5933\n",
      "step: 26700/50000 ...  - loss: 0.0466 - acc: 0.5949\n",
      "step: 26800/50000 ...  - loss: 0.0471 - acc: 0.5910\n",
      "step: 26900/50000 ...  - loss: 0.0468 - acc: 0.5941\n",
      "step: 27000/50000 ...  - loss: 0.0466 - acc: 0.5939\n",
      "step: 27100/50000 ...  - loss: 0.0467 - acc: 0.5928\n",
      "step: 27200/50000 ...  - loss: 0.0468 - acc: 0.5930\n",
      "step: 27300/50000 ...  - loss: 0.0469 - acc: 0.5924\n",
      "step: 27400/50000 ...  - loss: 0.0466 - acc: 0.5935\n",
      "step: 27500/50000 ...  - loss: 0.0467 - acc: 0.5936\n",
      "step: 27600/50000 ...  - loss: 0.0467 - acc: 0.5930\n",
      "step: 27700/50000 ...  - loss: 0.0465 - acc: 0.5936\n",
      "step: 27800/50000 ...  - loss: 0.0470 - acc: 0.5920\n",
      "step: 27900/50000 ...  - loss: 0.0467 - acc: 0.5941\n",
      "step: 28000/50000 ...  - loss: 0.0466 - acc: 0.5935\n",
      "step: 28100/50000 ...  - loss: 0.0467 - acc: 0.5939\n",
      "step: 28200/50000 ...  - loss: 0.0467 - acc: 0.5938\n",
      "step: 28300/50000 ...  - loss: 0.0465 - acc: 0.5942\n",
      "step: 28400/50000 ...  - loss: 0.0465 - acc: 0.5944\n",
      "step: 28500/50000 ...  - loss: 0.0466 - acc: 0.5940\n",
      "step: 28600/50000 ...  - loss: 0.0466 - acc: 0.5941\n",
      "step: 28700/50000 ...  - loss: 0.0465 - acc: 0.5945\n",
      "step: 28800/50000 ...  - loss: 0.0466 - acc: 0.5942\n",
      "step: 28900/50000 ...  - loss: 0.0467 - acc: 0.5932\n",
      "step: 29000/50000 ...  - loss: 0.0466 - acc: 0.5939\n",
      "step: 29100/50000 ...  - loss: 0.0467 - acc: 0.5932\n",
      "step: 29200/50000 ...  - loss: 0.0464 - acc: 0.5950\n",
      "step: 29300/50000 ...  - loss: 0.0464 - acc: 0.5943\n",
      "step: 29400/50000 ...  - loss: 0.0466 - acc: 0.5932\n",
      "step: 29500/50000 ...  - loss: 0.0464 - acc: 0.5945\n",
      "step: 29600/50000 ...  - loss: 0.0464 - acc: 0.5948\n",
      "step: 29700/50000 ...  - loss: 0.0465 - acc: 0.5935\n",
      "step: 29800/50000 ...  - loss: 0.0465 - acc: 0.5940\n",
      "step: 29900/50000 ...  - loss: 0.0464 - acc: 0.5943\n",
      "step: 30000/50000 ...  - loss: 0.0465 - acc: 0.5934\n",
      "step: 30100/50000 ...  - loss: 0.0464 - acc: 0.5943\n",
      "step: 30200/50000 ...  - loss: 0.0465 - acc: 0.5943\n",
      "step: 30300/50000 ...  - loss: 0.0465 - acc: 0.5936\n",
      "step: 30400/50000 ...  - loss: 0.0464 - acc: 0.5934\n",
      "step: 30500/50000 ...  - loss: 0.0464 - acc: 0.5949\n",
      "step: 30600/50000 ...  - loss: 0.0465 - acc: 0.5937\n",
      "step: 30700/50000 ...  - loss: 0.0465 - acc: 0.5937\n",
      "step: 30800/50000 ...  - loss: 0.0463 - acc: 0.5946\n",
      "step: 30900/50000 ...  - loss: 0.0464 - acc: 0.5939\n",
      "step: 31000/50000 ...  - loss: 0.0463 - acc: 0.5946\n",
      "step: 31100/50000 ...  - loss: 0.0465 - acc: 0.5941\n",
      "step: 31200/50000 ...  - loss: 0.0463 - acc: 0.5955\n",
      "step: 31300/50000 ...  - loss: 0.0462 - acc: 0.5955\n",
      "step: 31400/50000 ...  - loss: 0.0463 - acc: 0.5947\n",
      "step: 31500/50000 ...  - loss: 0.0463 - acc: 0.5941\n",
      "step: 31600/50000 ...  - loss: 0.0462 - acc: 0.5962\n",
      "step: 31700/50000 ...  - loss: 0.0464 - acc: 0.5954\n",
      "step: 31800/50000 ...  - loss: 0.0463 - acc: 0.5951\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 31900/50000 ...  - loss: 0.0463 - acc: 0.5951\n",
      "step: 32000/50000 ...  - loss: 0.0464 - acc: 0.5950\n",
      "step: 32100/50000 ...  - loss: 0.0463 - acc: 0.5955\n",
      "step: 32200/50000 ...  - loss: 0.0464 - acc: 0.5941\n",
      "step: 32300/50000 ...  - loss: 0.0464 - acc: 0.5941\n",
      "step: 32400/50000 ...  - loss: 0.0463 - acc: 0.5947\n",
      "step: 32500/50000 ...  - loss: 0.0462 - acc: 0.5959\n",
      "step: 32600/50000 ...  - loss: 0.0462 - acc: 0.5952\n",
      "step: 32700/50000 ...  - loss: 0.0462 - acc: 0.5952\n",
      "step: 32800/50000 ...  - loss: 0.0461 - acc: 0.5957\n",
      "step: 32900/50000 ...  - loss: 0.0464 - acc: 0.5941\n",
      "step: 33000/50000 ...  - loss: 0.0459 - acc: 0.5972\n",
      "step: 33100/50000 ...  - loss: 0.0461 - acc: 0.5955\n",
      "step: 33200/50000 ...  - loss: 0.0463 - acc: 0.5951\n",
      "step: 33300/50000 ...  - loss: 0.0463 - acc: 0.5948\n",
      "step: 33400/50000 ...  - loss: 0.0460 - acc: 0.5963\n",
      "step: 33500/50000 ...  - loss: 0.0460 - acc: 0.5959\n",
      "step: 33600/50000 ...  - loss: 0.0459 - acc: 0.5963\n",
      "step: 33700/50000 ...  - loss: 0.0461 - acc: 0.5953\n",
      "step: 33800/50000 ...  - loss: 0.0461 - acc: 0.5955\n",
      "step: 33900/50000 ...  - loss: 0.0462 - acc: 0.5942\n",
      "step: 34000/50000 ...  - loss: 0.0462 - acc: 0.5957\n",
      "step: 34100/50000 ...  - loss: 0.0460 - acc: 0.5962\n",
      "step: 34200/50000 ...  - loss: 0.0460 - acc: 0.5959\n",
      "step: 34300/50000 ...  - loss: 0.0461 - acc: 0.5957\n",
      "step: 34400/50000 ...  - loss: 0.0463 - acc: 0.5940\n",
      "step: 34500/50000 ...  - loss: 0.0459 - acc: 0.5965\n",
      "step: 34600/50000 ...  - loss: 0.0462 - acc: 0.5936\n",
      "step: 34700/50000 ...  - loss: 0.0459 - acc: 0.5962\n",
      "step: 34800/50000 ...  - loss: 0.0459 - acc: 0.5962\n",
      "step: 34900/50000 ...  - loss: 0.0460 - acc: 0.5959\n",
      "step: 35000/50000 ...  - loss: 0.0459 - acc: 0.5970\n",
      "step: 35100/50000 ...  - loss: 0.0459 - acc: 0.5959\n",
      "step: 35200/50000 ...  - loss: 0.0461 - acc: 0.5951\n",
      "step: 35300/50000 ...  - loss: 0.0460 - acc: 0.5967\n",
      "step: 35400/50000 ...  - loss: 0.0462 - acc: 0.5944\n",
      "step: 35500/50000 ...  - loss: 0.0459 - acc: 0.5952\n",
      "step: 35600/50000 ...  - loss: 0.0459 - acc: 0.5962\n",
      "step: 35700/50000 ...  - loss: 0.0461 - acc: 0.5951\n",
      "step: 35800/50000 ...  - loss: 0.0458 - acc: 0.5972\n",
      "step: 35900/50000 ...  - loss: 0.0461 - acc: 0.5952\n",
      "step: 36000/50000 ...  - loss: 0.0459 - acc: 0.5967\n",
      "step: 36100/50000 ...  - loss: 0.0458 - acc: 0.5960\n",
      "step: 36200/50000 ...  - loss: 0.0458 - acc: 0.5963\n",
      "step: 36300/50000 ...  - loss: 0.0459 - acc: 0.5955\n",
      "step: 36400/50000 ...  - loss: 0.0457 - acc: 0.5968\n",
      "step: 36500/50000 ...  - loss: 0.0461 - acc: 0.5944\n",
      "step: 36600/50000 ...  - loss: 0.0456 - acc: 0.5974\n",
      "step: 36700/50000 ...  - loss: 0.0459 - acc: 0.5969\n",
      "step: 36800/50000 ...  - loss: 0.0457 - acc: 0.5970\n",
      "step: 36900/50000 ...  - loss: 0.0460 - acc: 0.5955\n",
      "step: 37000/50000 ...  - loss: 0.0459 - acc: 0.5952\n",
      "step: 37100/50000 ...  - loss: 0.0457 - acc: 0.5969\n",
      "step: 37200/50000 ...  - loss: 0.0457 - acc: 0.5971\n",
      "step: 37300/50000 ...  - loss: 0.0457 - acc: 0.5966\n",
      "step: 37400/50000 ...  - loss: 0.0460 - acc: 0.5962\n",
      "step: 37500/50000 ...  - loss: 0.0459 - acc: 0.5949\n",
      "step: 37600/50000 ...  - loss: 0.0458 - acc: 0.5968\n",
      "step: 37700/50000 ...  - loss: 0.0459 - acc: 0.5955\n",
      "step: 37800/50000 ...  - loss: 0.0458 - acc: 0.5961\n",
      "step: 37900/50000 ...  - loss: 0.0457 - acc: 0.5963\n",
      "step: 38000/50000 ...  - loss: 0.0456 - acc: 0.5976\n",
      "step: 38100/50000 ...  - loss: 0.0458 - acc: 0.5965\n",
      "step: 38200/50000 ...  - loss: 0.0457 - acc: 0.5974\n",
      "step: 38300/50000 ...  - loss: 0.0458 - acc: 0.5956\n",
      "step: 38400/50000 ...  - loss: 0.0458 - acc: 0.5970\n",
      "step: 38500/50000 ...  - loss: 0.0459 - acc: 0.5947\n",
      "step: 38600/50000 ...  - loss: 0.0457 - acc: 0.5973\n",
      "step: 38700/50000 ...  - loss: 0.0456 - acc: 0.5969\n",
      "step: 38800/50000 ...  - loss: 0.0455 - acc: 0.5974\n",
      "step: 38900/50000 ...  - loss: 0.0455 - acc: 0.5964\n",
      "step: 39000/50000 ...  - loss: 0.0456 - acc: 0.5970\n",
      "step: 39100/50000 ...  - loss: 0.0458 - acc: 0.5961\n",
      "step: 39200/50000 ...  - loss: 0.0455 - acc: 0.5975\n",
      "step: 39300/50000 ...  - loss: 0.0456 - acc: 0.5974\n",
      "step: 39400/50000 ...  - loss: 0.0458 - acc: 0.5964\n",
      "step: 39500/50000 ...  - loss: 0.0456 - acc: 0.5972\n",
      "step: 39600/50000 ...  - loss: 0.0455 - acc: 0.5971\n",
      "step: 39700/50000 ...  - loss: 0.0456 - acc: 0.5966\n",
      "step: 39800/50000 ...  - loss: 0.0455 - acc: 0.5982\n",
      "step: 39900/50000 ...  - loss: 0.0455 - acc: 0.5979\n",
      "step: 40000/50000 ...  - loss: 0.0456 - acc: 0.5977\n",
      "step: 40100/50000 ...  - loss: 0.0455 - acc: 0.5977\n",
      "step: 40200/50000 ...  - loss: 0.0456 - acc: 0.5972\n",
      "step: 40300/50000 ...  - loss: 0.0455 - acc: 0.5982\n",
      "step: 40400/50000 ...  - loss: 0.0455 - acc: 0.5975\n",
      "step: 40500/50000 ...  - loss: 0.0455 - acc: 0.5977\n",
      "step: 40600/50000 ...  - loss: 0.0457 - acc: 0.5960\n",
      "step: 40700/50000 ...  - loss: 0.0454 - acc: 0.5982\n",
      "step: 40800/50000 ...  - loss: 0.0455 - acc: 0.5974\n",
      "step: 40900/50000 ...  - loss: 0.0455 - acc: 0.5972\n",
      "step: 41000/50000 ...  - loss: 0.0452 - acc: 0.5997\n",
      "step: 41100/50000 ...  - loss: 0.0457 - acc: 0.5963\n",
      "step: 41200/50000 ...  - loss: 0.0455 - acc: 0.5975\n",
      "step: 41300/50000 ...  - loss: 0.0454 - acc: 0.5971\n",
      "step: 41400/50000 ...  - loss: 0.0453 - acc: 0.5992\n",
      "step: 41500/50000 ...  - loss: 0.0453 - acc: 0.5983\n",
      "step: 41600/50000 ...  - loss: 0.0453 - acc: 0.5976\n",
      "step: 41700/50000 ...  - loss: 0.0454 - acc: 0.5973\n",
      "step: 41800/50000 ...  - loss: 0.0454 - acc: 0.5986\n",
      "step: 41900/50000 ...  - loss: 0.0455 - acc: 0.5971\n",
      "step: 42000/50000 ...  - loss: 0.0454 - acc: 0.5982\n",
      "step: 42100/50000 ...  - loss: 0.0454 - acc: 0.5970\n",
      "step: 42200/50000 ...  - loss: 0.0454 - acc: 0.5969\n",
      "step: 42300/50000 ...  - loss: 0.0454 - acc: 0.5972\n",
      "step: 42400/50000 ...  - loss: 0.0453 - acc: 0.5988\n",
      "step: 42500/50000 ...  - loss: 0.0454 - acc: 0.5973\n",
      "step: 42600/50000 ...  - loss: 0.0453 - acc: 0.5983\n",
      "step: 42700/50000 ...  - loss: 0.0452 - acc: 0.5986\n",
      "step: 42800/50000 ...  - loss: 0.0451 - acc: 0.5995\n",
      "step: 42900/50000 ...  - loss: 0.0455 - acc: 0.5970\n",
      "step: 43000/50000 ...  - loss: 0.0452 - acc: 0.5990\n",
      "step: 43100/50000 ...  - loss: 0.0451 - acc: 0.5995\n",
      "step: 43200/50000 ...  - loss: 0.0454 - acc: 0.5976\n",
      "step: 43300/50000 ...  - loss: 0.0451 - acc: 0.5988\n",
      "step: 43400/50000 ...  - loss: 0.0454 - acc: 0.5972\n",
      "step: 43500/50000 ...  - loss: 0.0453 - acc: 0.5982\n",
      "step: 43600/50000 ...  - loss: 0.0451 - acc: 0.5986\n",
      "step: 43700/50000 ...  - loss: 0.0451 - acc: 0.5986\n",
      "step: 43800/50000 ...  - loss: 0.0454 - acc: 0.5970\n",
      "step: 43900/50000 ...  - loss: 0.0453 - acc: 0.5973\n",
      "step: 44000/50000 ...  - loss: 0.0453 - acc: 0.5969\n",
      "step: 44100/50000 ...  - loss: 0.0453 - acc: 0.5977\n",
      "step: 44200/50000 ...  - loss: 0.0452 - acc: 0.5985\n",
      "step: 44300/50000 ...  - loss: 0.0451 - acc: 0.5989\n",
      "step: 44400/50000 ...  - loss: 0.0451 - acc: 0.5982\n",
      "step: 44500/50000 ...  - loss: 0.0452 - acc: 0.5986\n",
      "step: 44600/50000 ...  - loss: 0.0451 - acc: 0.5989\n",
      "step: 44700/50000 ...  - loss: 0.0451 - acc: 0.5989\n",
      "step: 44800/50000 ...  - loss: 0.0449 - acc: 0.6003\n",
      "step: 44900/50000 ...  - loss: 0.0452 - acc: 0.5969\n",
      "step: 45000/50000 ...  - loss: 0.0450 - acc: 0.5995\n",
      "step: 45100/50000 ...  - loss: 0.0453 - acc: 0.5971\n",
      "step: 45200/50000 ...  - loss: 0.0452 - acc: 0.5981\n",
      "step: 45300/50000 ...  - loss: 0.0450 - acc: 0.5993\n",
      "step: 45400/50000 ...  - loss: 0.0450 - acc: 0.5986\n",
      "step: 45500/50000 ...  - loss: 0.0451 - acc: 0.5986\n",
      "step: 45600/50000 ...  - loss: 0.0450 - acc: 0.5994\n",
      "step: 45700/50000 ...  - loss: 0.0450 - acc: 0.5991\n",
      "step: 45800/50000 ...  - loss: 0.0451 - acc: 0.5982\n",
      "step: 45900/50000 ...  - loss: 0.0450 - acc: 0.5983\n",
      "step: 46000/50000 ...  - loss: 0.0450 - acc: 0.5988\n",
      "step: 46100/50000 ...  - loss: 0.0451 - acc: 0.5979\n",
      "step: 46200/50000 ...  - loss: 0.0450 - acc: 0.5995\n",
      "step: 46300/50000 ...  - loss: 0.0450 - acc: 0.5991\n",
      "step: 46400/50000 ...  - loss: 0.0448 - acc: 0.5996\n",
      "step: 46500/50000 ...  - loss: 0.0449 - acc: 0.5996\n",
      "step: 46600/50000 ...  - loss: 0.0450 - acc: 0.5988\n",
      "step: 46700/50000 ...  - loss: 0.0449 - acc: 0.5994\n",
      "step: 46800/50000 ...  - loss: 0.0449 - acc: 0.5992\n",
      "step: 46900/50000 ...  - loss: 0.0450 - acc: 0.5983\n",
      "step: 47000/50000 ...  - loss: 0.0449 - acc: 0.5987\n",
      "step: 47100/50000 ...  - loss: 0.0450 - acc: 0.5988\n",
      "step: 47200/50000 ...  - loss: 0.0451 - acc: 0.5980\n",
      "step: 47300/50000 ...  - loss: 0.0448 - acc: 0.6004\n",
      "step: 47400/50000 ...  - loss: 0.0449 - acc: 0.5988\n",
      "step: 47500/50000 ...  - loss: 0.0451 - acc: 0.5974\n",
      "step: 47600/50000 ...  - loss: 0.0448 - acc: 0.5993\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 47700/50000 ...  - loss: 0.0449 - acc: 0.5994\n",
      "step: 47800/50000 ...  - loss: 0.0449 - acc: 0.5987\n",
      "step: 47900/50000 ...  - loss: 0.0449 - acc: 0.5984\n",
      "step: 48000/50000 ...  - loss: 0.0449 - acc: 0.5985\n",
      "step: 48100/50000 ...  - loss: 0.0448 - acc: 0.5995\n",
      "step: 48200/50000 ...  - loss: 0.0449 - acc: 0.5993\n",
      "step: 48300/50000 ...  - loss: 0.0447 - acc: 0.6002\n",
      "step: 48400/50000 ...  - loss: 0.0447 - acc: 0.5998\n",
      "step: 48500/50000 ...  - loss: 0.0449 - acc: 0.5984\n",
      "step: 48600/50000 ...  - loss: 0.0447 - acc: 0.5996\n",
      "step: 48700/50000 ...  - loss: 0.0446 - acc: 0.6003\n",
      "step: 48800/50000 ...  - loss: 0.0449 - acc: 0.5984\n",
      "step: 48900/50000 ...  - loss: 0.0450 - acc: 0.5984\n",
      "step: 49000/50000 ...  - loss: 0.0448 - acc: 0.5993\n",
      "step: 49100/50000 ...  - loss: 0.0449 - acc: 0.5986\n",
      "step: 49200/50000 ...  - loss: 0.0447 - acc: 0.5994\n",
      "step: 49300/50000 ...  - loss: 0.0447 - acc: 0.6002\n",
      "step: 49400/50000 ...  - loss: 0.0449 - acc: 0.5981\n",
      "step: 49500/50000 ...  - loss: 0.0447 - acc: 0.6001\n",
      "step: 49600/50000 ...  - loss: 0.0448 - acc: 0.5996\n",
      "step: 49700/50000 ...  - loss: 0.0448 - acc: 0.6002\n",
      "step: 49800/50000 ...  - loss: 0.0448 - acc: 0.5996\n",
      "step: 49900/50000 ...  - loss: 0.0445 - acc: 0.6008\n",
      "step: 50000/50000 ...  - loss: 0.0449 - acc: 0.5981\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fba48302dd8>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(inputs, outputs,\n",
    "    batch_size=6400,\n",
    "    epochs=50000,\n",
    "    verbose=0,\n",
    "    shuffle=True,\n",
    "    validation_split=0.0,\n",
    "    callbacks=[NBatchLogger(display=100)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.084226996 -0.076150835 -0.119199276\n",
      "0.08166539999999997 -0.14852280000000065 0.4672567999999956\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#score = model.evaluate(x_test, y_test, verbose=0)a\n",
    "#print('Test loss:', score[0])\n",
    "#print('Test accuracy:', score[1])\n",
    "\n",
    "ind=535\n",
    "rx,ry,rz = outputs[ind]\n",
    "x,y,z, = model.predict(np.array([inputs[ind]]))[0]\n",
    "\n",
    "print(x,y,z)\n",
    "print(rx,ry,rz)\n",
    "#print(((rx-x)**2+(ry-y)**2+(rz-z)**2))\n",
    "#print(inputs[10])\n",
    "#print(outputs[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    0.63964521     0.16337221    -0.47358871     0.58298998\n",
      "   -96.314989     190.194505   -1002.692322       0.\n",
      "     0.          -300.             0.        ]\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(suppress=True)\n",
    "print(inputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
